{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3P4e-lrROcM"
      },
      "outputs": [],
      "source": [
        "def read_sentences_from_file(filename):\n",
        "    with open(filename, 'r', encoding=\"utf-8\") as file:\n",
        "        return [line.strip() for line in file]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXOJs_xFSfgb"
      },
      "source": [
        "Creating dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsjsiVsvR8z3"
      },
      "outputs": [],
      "source": [
        "valid = read_sentences_from_file('dataset/valid.txt')\n",
        "\n",
        "invalid = read_sentences_from_file('dataset/invalid.txt')\n",
        "\n",
        "valid = valid[:3000000]\n",
        "\n",
        "invalid = invalid[:3000000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCT0SiZ7SuFb"
      },
      "source": [
        "Tokenizing words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMRpt00QSkFi"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "all_sentences = invalid + valid\n",
        "\n",
        "labels = np.array([0] * len(invalid) + [1] * len(valid))\n",
        "\n",
        "combined_data = list(zip(all_sentences, labels))\n",
        "np.random.shuffle(combined_data)\n",
        "all_sentences, labels = zip(*combined_data)\n",
        "\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(all_sentences, labels, test_size=0.2, random_state=42)\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_sentences, train_labels, test_size=0.1, random_state=42)\n",
        "\n",
        "tokenizer = Tokenizer(oov_token=\"None\")\n",
        "tokenizer.fit_on_texts(train_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "max_sequence_length = 100\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "train_sequences = pad_sequences(train_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "val_sequences = tokenizer.texts_to_sequences(val_sentences)\n",
        "val_sequences = pad_sequences(val_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "test_sequences = tokenizer.texts_to_sequences(test_sentences)\n",
        "test_sequences = pad_sequences(test_sequences, maxlen=max_sequence_length)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5C96ny7SxhL"
      },
      "source": [
        "Creating and training the deep learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urVTN5wYSqNT"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, GRU, Dense, Dropout, Bidirectional, LSTM\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define model\n",
        "model = Sequential(name=\"dl_model\")\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=64, name=\"embedding_layer\"))\n",
        "lstm_first = LSTM(64, return_sequences=True, name=\"lstm_first\")\n",
        "model.add(Bidirectional(lstm_first, name=\"bidirectional_lstm_first\"))\n",
        "lstm_second = LSTM(32, name=\"lstm_second\")\n",
        "model.add(Bidirectional(lstm_second, name=\"bidirectional_lstm_second\"))\n",
        "model.add(Dense(64, activation='relu', name=\"dense_first\"))\n",
        "model.add(Dropout(0.5, name=\"dropout_first\"))\n",
        "model.add(Dense(1, activation='sigmoid', name=\"output_layer\"))\n",
        "\n",
        "# Compile model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "train_sequences = np.array(train_sequences)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Train model\n",
        "history = model.fit(train_sequences, train_labels, epochs=4, batch_size=64, verbose=1,\n",
        "                    validation_split=0.2)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2P6nDIeTAoK"
      },
      "source": [
        "Loading the model and converting it into TensorFlow.js files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNpbnmfvTTUP"
      },
      "outputs": [],
      "source": [
        "model.save(\"deepLearning.h5\")\n",
        "!tensorflowjs_converter --input_format keras deepLearning.h5 finalDL"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
